/**
 * ChatAI External LLM API Client
 * 
 * TypeScript implementation for connecting to ChatAI external LLM API
 * Based on the chat.sh script functionality for web backend use.
 */

import axios, { AxiosInstance, AxiosResponse } from 'axios';
import https from 'https';

export interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface ChatCompletionRequest {
  model: string;
  messages: ChatMessage[];
  temperature?: number;
  stream?: boolean;
  response_format?: {
    type: "json_object" | "text";
  };
  response_mime_type?: string;
}

export interface ChatCompletionResponse {
  choices: Array<{
    message: {
      content: string;
      role: string;
    };
  }>;
}

export interface ModelInfo {
  id: string;
  owned_by: string;
}

export interface ModelsResponse {
  data: ModelInfo[];
}

export type APIDialect = 'openai' | 'openai_raw' | 'ollama_native';

export interface ChatAIClientConfig {
  apiKey: string;
  baseURL: string;
  allowInsecure?: boolean;
  timeout?: number;
}

export class ChatAIClient {
  private client: AxiosInstance;
  private apiDialect: APIDialect = 'openai';
  private readonly apiKey: string;
  private readonly baseURL: string;

  constructor(config: ChatAIClientConfig) {
    this.apiKey = config.apiKey;
    this.baseURL = config.baseURL.replace(/\/$/, '');

    this.client = axios.create({
      baseURL: this.baseURL,
      timeout: config.timeout || 60000,
      headers: {
        'Content-Type': 'application/json',
      },
      httpsAgent: config.allowInsecure ? new https.Agent({
        rejectUnauthorized: false
      }) : undefined,
    });
  }

  private buildApiUrl(endpoint: string): string {
    const basePath = this.baseURL;
    
    console.log("ğŸ—ï¸ å»ºæ§‹ API URL:");
    console.log("ğŸ”§ API Dialect:", this.apiDialect);
    console.log("ğŸ  Base URL:", basePath);
    console.log("ğŸ“ Endpoint:", endpoint);
    
    let finalUrl: string;
    switch (this.apiDialect) {
      case 'ollama_native':
        const ollamaEndpoints: Record<string, string> = {
          'models': '/api/tags',
          'chat/completions': '/api/chat',
          'embeddings': '/api/embeddings'
        };
        finalUrl = `${basePath}${ollamaEndpoints[endpoint] || endpoint}`;
        break;
      
      case 'openai_raw':
        finalUrl = `${basePath}/${endpoint}`;
        break;
      
      case 'openai':
      default:
        finalUrl = `${basePath}/v1/${endpoint}`;
        break;
    }
    
    console.log("ğŸŒ æœ€çµ‚ URL:", finalUrl);
    return finalUrl;
  }

  private getAuthHeaders(): Record<string, string> {
    if (this.apiDialect === 'ollama_native') {
      return {};
    }
    return {
      'Authorization': `Bearer ${this.apiKey}`
    };
  }

  /**
   * Fetch available models with automatic dialect detection
   * Tries OpenAI v1, then raw, then Ollama native endpoints
   */
  async getModels(): Promise<ModelInfo[]> {
    // å„ªå…ˆå˜—è©¦ OpenAI v1 è·¯å¾‘
    try {
      this.apiDialect = 'openai';
      const url = this.buildApiUrl('models');
      console.log(`ğŸ” å˜—è©¦ OpenAI v1 è·¯å¾‘: ${url}`);
      
      const response: AxiosResponse<ModelsResponse> = await this.client.get(url, {
        headers: this.getAuthHeaders()
      });
      
      if (response.data.data && Array.isArray(response.data.data)) {
        console.log('âœ… OpenAI v1 è·¯å¾‘æˆåŠŸ');
        return response.data.data;
      }
    } catch (error) {
      console.warn('âš ï¸ OpenAI v1 è·¯å¾‘å¤±æ•—ï¼Œå˜—è©¦åŸå§‹è·¯å¾‘...');
    }

    // å˜—è©¦ OpenAI åŸå§‹è·¯å¾‘
    try {
      this.apiDialect = 'openai_raw';
      const url = this.buildApiUrl('models');
      console.log(`ğŸ” å˜—è©¦ OpenAI åŸå§‹è·¯å¾‘: ${url}`);
      
      const response: AxiosResponse<ModelsResponse> = await this.client.get(url, {
        headers: this.getAuthHeaders()
      });
      
      if (response.data.data && Array.isArray(response.data.data)) {
        console.log('âœ… OpenAI åŸå§‹è·¯å¾‘æˆåŠŸ');
        return response.data.data;
      }
    } catch (error) {
      console.warn('âš ï¸ OpenAI åŸå§‹è·¯å¾‘å¤±æ•—ï¼Œå˜—è©¦ Ollama åŸç”Ÿ...');
    }

    // å˜—è©¦ Ollama åŸç”Ÿè·¯å¾‘
    try {
      this.apiDialect = 'ollama_native';
      const url = this.buildApiUrl('models');
      console.log(`ğŸ” å˜—è©¦ Ollama åŸç”Ÿè·¯å¾‘: ${url}`);
      
      const response = await this.client.get(url, {
        headers: this.getAuthHeaders()
      });
      
      if (response.data.models && Array.isArray(response.data.models)) {
        console.log('âœ… Ollama åŸç”Ÿè·¯å¾‘æˆåŠŸï¼Œæ­£åœ¨è½‰æ›æ ¼å¼...');
        // è½‰æ› Ollama æ ¼å¼ç‚º OpenAI ç›¸å®¹æ ¼å¼
        return response.data.models.map((model: any) => ({
          id: model.name,
          owned_by: 'ollama'
        }));
      }
    } catch (error) {
      console.error('âŒ Ollama åŸç”Ÿè·¯å¾‘å¤±æ•—:', error);
    }

    // All attempts failed
    this.apiDialect = 'openai'; // Reset to default
    throw new Error('âŒ ç„¡æ³•å¾æ‰€æœ‰å¯ç”¨ç«¯é» (v1, raw, ollama) ç²å–æ¨¡å‹');
  }

  /**
   * Perform chat completion request
   * Returns the content of the response message
   */
  async chatCompletion(request: ChatCompletionRequest): Promise<string> {
    const url = this.buildApiUrl('chat/completions');
    
    console.log("ğŸŒ ChatAI API å‘¼å«è©³æƒ…:");
    console.log("ğŸ“¡ URL:", url);
    console.log("ğŸ”§ API Dialect:", this.apiDialect);
    console.log("ğŸ¯ Request:", {
      model: request.model,
      messagesCount: request.messages?.length,
      temperature: request.temperature,
      response_format: request.response_format,
      response_mime_type: request.response_mime_type,
    });
    console.log("ğŸ”‘ Headers:", {
      ...this.getAuthHeaders(),
      Authorization: this.getAuthHeaders().Authorization ? '[HIDDEN]' : 'None'
    });
    
    // è®°å½•å®Œæ•´çš„æ¶ˆæ¯å†…å®¹
    console.log("ğŸ“ å®Œæ•´è¯·æ±‚æ¶ˆæ¯:");
    console.log("=".repeat(100));
    request.messages.forEach((message, index) => {
      console.log(`[æ¶ˆæ¯ ${index + 1}] è§’è‰²: ${message.role}`);
      console.log(`[æ¶ˆæ¯ ${index + 1}] å†…å®¹é•¿åº¦: ${message.content.length} å­—ç¬¦`);
      console.log(`[æ¶ˆæ¯ ${index + 1}] å†…å®¹:`);
      console.log(message.content);
      console.log("-".repeat(80));
    });
    console.log("=".repeat(100));
    
    try {
      const response: AxiosResponse<ChatCompletionResponse> = await this.client.post(url, request, {
        headers: this.getAuthHeaders()
      });

      if (this.apiDialect === 'ollama_native') {
        // Ollama native response format
        return (response.data as any).message?.content || '';
      } else {
        // OpenAI compatible response format
        return response.data.choices[0]?.message?.content || '';
      }
    } catch (error: any) {
      console.error('âŒ èŠå¤©å®Œæˆå¤±æ•—:', error);
      
      if (error.response) {
        console.error('ğŸ” å›æ‡‰éŒ¯èª¤è©³æƒ…:');
        console.error('ğŸ“Š ç‹€æ…‹:', error.response.status);
        console.error('ğŸ“ ç‹€æ…‹æ–‡å­—:', error.response.statusText);
        console.error('ğŸ—‚ï¸ æ¨™é ­:', error.response.headers);
        console.error('ğŸ“„ è³‡æ–™:', error.response.data);
        console.error('ğŸŒ è«‹æ±‚ URL:', error.config?.url);
        console.error('ğŸ”§ è«‹æ±‚æ–¹æ³•:', error.config?.method?.toUpperCase());
      } else if (error.request) {
        console.error('ğŸš« æœªæ”¶åˆ°å›æ‡‰:');
        console.error('ğŸ“¡ è«‹æ±‚:', error.request);
      } else {
        console.error('âš™ï¸ è«‹æ±‚è¨­å®šéŒ¯èª¤:', error.message);
      }
      
      throw new Error(`èŠå¤©å®Œæˆå¤±æ•—: ${error.message || error}`);
    }
  }

  /**
   * Perform streaming chat completion request
   * Calls onChunk for each content chunk and onComplete when done
   */
  async chatCompletionStream(
    request: ChatCompletionRequest,
    onChunk: (content: string) => void,
    onComplete: () => void
  ): Promise<void> {
    const url = this.buildApiUrl('chat/completions');
    const streamRequest = { ...request, stream: true };

    try {
      const response = await this.client.post(url, streamRequest, {
        headers: this.getAuthHeaders(),
        responseType: 'stream'
      });

      response.data.on('data', (chunk: Buffer) => {
        const lines = chunk.toString().split('\n');
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            
            if (data === '[DONE]') {
              onComplete();
              return;
            }

            try {
              const parsed = JSON.parse(data);
              
              if (this.apiDialect === 'ollama_native') {
                if (parsed.done) {
                  onComplete();
                  return;
                }
                const content = parsed.message?.content;
                if (content) onChunk(content);
              } else {
                const content = parsed.choices?.[0]?.delta?.content;
                if (content) onChunk(content);
              }
            } catch (parseError) {
              // Skip invalid JSON chunks
            }
          }
        }
      });

      response.data.on('end', () => {
        onComplete();
      });

      response.data.on('error', (error: any) => {
        console.error('âŒ Stream error:', error);
        throw error;
      });

    } catch (error) {
      console.error('âŒ Stream chat completion failed:', error);
      throw new Error(`Stream chat completion failed: ${error}`);
    }
  }

  /**
   * Get text embeddings
   * Returns the raw embedding response
   */
  async embeddings(model: string, inputText: string): Promise<any> {
    const url = this.buildApiUrl('embeddings');
    
    const payload = {
      model: model,
      input: inputText
    };

    try {
      const response = await this.client.post(url, payload, {
        headers: this.getAuthHeaders()
      });
      
      return response.data;
    } catch (error) {
      console.error('âŒ Embeddings request failed:', error);
      throw new Error(`Embeddings request failed: ${error}`);
    }
  }
}

/**
 * Factory function to create ChatAI client instance
 */
export function createChatAIClient(config: ChatAIClientConfig): ChatAIClient {
  return new ChatAIClient(config);
}

/**
 * Example usage:
 * 
 * const client = createChatAIClient({
 *   apiKey: "sk-NXEfbTnSFxmsDljpeSDTmb3yFN9wyeX7WU7NOs8hzfYYjogp",
 *   baseURL: "https://www.chataiapi.com"
 * });
 * 
 * // ç²å–å¯ç”¨æ¨¡å‹
 * const models = await client.getModels();
 * console.log('Available models:', models.map(m => m.id));
 * 
 * // èŠå¤©å®Œæˆ
 * const response = await client.chatCompletion({
 *   model: "gpt-3.5-turbo",
 *   messages: [{ role: "user", content: "Hello!" }],
 *   temperature: 0.7
 * });
 * console.log('Response:', response);
 * 
 * // æµå¼èŠå¤©
 * await client.chatCompletionStream({
 *   model: "gpt-3.5-turbo", 
 *   messages: [{ role: "user", content: "Hello!" }],
 *   temperature: 0.7
 * }, 
 * (chunk) => process.stdout.write(chunk),
 * () => console.log('\n[Complete]')
 * );
 */